<!DOCTYPE html>
<html lang="en">
<head>
  <title>OmniSat</title>
  <meta name="description" content="OmniSat: Self-Supervised Modality Fusion for Earth Observation.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <meta property="og:image" content="https://github.com/gastruc/OmniSat/assets/1902679/9fc20951-1cac-4891-b67f-53ed5e0675ad">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://gastruc.github.io/projects/omnisat.html/"/>
  <meta property="og:title" content="OmniSat"/>
  <meta property="og:description" content="Project page for OmniSat: Self-Supervised Modality Fusion for Earth Observation."/>

  <!--Twitter preview-->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="OmniSat: Self-Supervised Modality Fusion for Earth Observation" />
  <meta name="twitter:description" content="Project page for OmniSat: Self-Supervised Modality Fusion for Earth Observation."/>
  <meta name="twitter:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail_twitter.png">

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>OmniSat: Self-Supervised Modality Fusion for Earth Observation</h1>
    <h4>ECCV 2024</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>
        <a href="https://gastruc.github.io/"><nobr>Guillaume Astruc</nobr></a> &emsp;
        <a href="https://ngonthier.github.io/"><nobr>Nicolas Gonthier</nobr></a> &emsp;
        <a href="https://www.umr-lastig.fr/clement-mallet/"><nobr>Clément Mallet</nobr></a> &emsp;
        <a href="https://loiclandrieu.com/"><nobr>Loic Landrieu</nobr></a>
      </h4>
      LIGM, <nobr>&Eacute;cole des Ponts</nobr>, <nobr>Univ Gustave Eiffel</nobr>, CNRS,
      <nobr>Marne-la-Vall&eacute;e, France</nobr>
      Univ Gustave Eiffel, IGN, ENSG, LASTIG, France
      IGN, France
      CNES, France
    </div>
    <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
      <a href="https://arxiv.org/pdf/2006.11132.pdf" style="color:inherit">
        <i class="fa fa-file-pdf-o fa-4x"></i></a> 
    </div>
    <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
      <a href="https://github.com/gastruc/OmniSat" style="color:inherit">
        <i class="fa fa-github fa-4x"></i></a>
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="https://github.com/gastruc/OmniSat/assets/1902679/9fc20951-1cac-4891-b67f-53ed5e0675ad" alt="teaser.jpg" class="text-center" style="width: 60%; max-width: 1100px; height: auto;">
  <h3 style="text-align:center; padding-top:1rem">
    <a class="label label-info" href="https://arxiv.org/pdf/2404.08351">Paper</a>
    <a class="label label-info" href="https://github.com/gastruc/OmniSat">Code</a>
    <!-- <a class="label label-info" href="resrc/ref.bib">BibTeX</a> -->
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    The diversity and complementarity of sensors available for Earth Observations (EO) calls for developing bespoke self-supervised multimodal learning 
    approaches. However, current multimodal EO datasets and models typically focus on a single data type, either mono-date images or time series, 
    which limits their impact. To address this issue, we introduce OmniSat, a novel architecture able to merge diverse EO modalities into <b>expressive 
    features without labels by exploiting their alignment</b>. To demonstrate the advantages of our approach, we create two new multimodal datasets 
    by augmenting existing ones with new modalities. As demonstrated for three downstream tasks---forestry, land cover classification, 
    and crop mapping---OmniSat can learn rich representations without supervision, leading to state-of-the-art performances in semi- and fully 
    supervised settings. Furthermore, our multimodal pretraining scheme improves performance <b>even when only one modality is available for inference</b>. 
  </p>

  <h3>Datasets</h3>
    <style>
    table {
      font-family: arial, sans-serif;
      border-collapse: collapse;
      width: 100%;
    }
    
    td, th {
      border: 1px solid #dddddd;
      text-align: left;
      padding: 2px;
    }
    
    tr:nth-child(1) {
      color: white;
      background-color: #003e5c;
    }
    </style>
    <table>
      <tbody>
        <tr>
          <th>Dataset name</th>
          <th>Modalities</th>
          <th>Labels</th>
          <th>Link</th>
        </tr>
        <tr>
          <td>PASTIS-HD</td>
          <td> <b>SPOT 6-7 (1m)</b> + S1/S2 (30-140 / year)</td>
          <td>Crop mapping (0.2m)</td>
          <td><a href="https://huggingface.co/datasets/IGNF/PASTIS-HD">huggingface</a> or <a href="https://zenodo.org/records/10908628">zenodo</a></td>
        </tr>
        <tr>
          <td>TreeSatAI-TS</td>
          <td>Aerial (0.2m) + <b>S1/S2 (10-70 / year)</b> </td>
          <td>Forestry (60m)</td>
          <td><a href="https://huggingface.co/datasets/IGNF/TreeSatAI-Time-Series">huggingface</a></td>
        </tr>
        <tr>
          <td>FLAIR</td>
          <td>aerial (0.2m) + S2 (20-114 / year)</td>
          <td>Land cover (0.2m)</td>
          <td><a href="https://huggingface.co/datasets/IGNF/FLAIR">huggingface</a></td>
        </tr>
      </tbody>
    </table>
    <br>
    </p> 
    We represent three tiles from the considered multilabel classification datasets:
    FLAIR (a), TreeSatAI-TS (b) and PASTIS-HD (c). TreeSatAI-TS is a new dataset built by replacing
    the single-date Sentinel-1 and 2 images of TreeSatAI by year-long time series. PASTIS-HD (c)
    adds VHR satellite images to PASTIS-R.
    </p>
    <p align="center">
      <img src="https://github.com/user-attachments/assets/18acbb19-6c90-4c9a-be05-0af24ded2052" width="700" height="350">
    </p>

  <h3>Results</h3>
    <p>We perform experiments with 100% and 10-20% of labels. When using all modalities, OmniSat outperforms all competing methods. 
      Our pre-training leads to more expressive multimodal features.
      See below, the F1 Score results on 100% of the training data with all modalities available:</p>
    <style>
      table, th, td {
        border: 2px solid black; /* Thicker borders */
        border-collapse: collapse;
        padding: 10px;  /* Increase padding for spacing */
      }
    </style>
    <table>
      <tbody>
        <tr>
          <th>F1 Score All Modalities</th>
          <th>UT&T</th>
          <th>Scale-MAE</th>
          <th>DOFA</th>
          <th>OmniSat (no pretraining)</th>
          <th>OmniSat (with pretraining)</th>
        </tr>
        <tr>
          <td>PASTIS-HD</td>
          <td>53.5</td>
          <td>42.2</td>
          <td>55.7</td>
          <td>59.1</td>
          <td style="font-weight: bold;">69.9</td>
        </tr>
        <tr>
          <td>TreeSatAI-TS</td>
          <td>56.7</td>
          <td>60.4</td>
          <td>71.3</td>
          <td>73.3</td>
          <td style="font-weight: bold;">74.2</td>
        </tr>
        <tr>
          <td>FLAIR</td>
          <td>48.8</td>
          <td>70.0</td>
          <td style="font-weight: bold;">74.9</td>
          <th>70.0</th>
          <td>73.4</td>
        </tr>
      </tbody>
    </table>
    <br>
    <p>OmniSat also improves performance even when only one modality is available for inference. Our self-supervised pre-training scheme 
      improves the features learned by each encoder despite not relying on annotated data.
      F1 Score results on 100% of the training data with only S2 data available:</p>
    
    <table>
      <tbody>
        <tr>
          <th>F1 Score S2 only</th>
          <th>UT&T</th>
          <th>Scale-MAE</th>
          <th>DOFA</th>
          <th>OmniSat (no pretraining)</th>
          <th>OmniSat (with pretraining)</th>
        </tr>
        <tr>
          <td>PASTIS-HD</td>
          <td>61.3</td>
          <td>46.1</td>
          <td>53.4</td>
          <td>60.1</td>
          <td style="font-weight: bold;">70.8</td>
        </tr>
        <tr>
          <td>TreeSatAI-TS</td>
          <td>57.0</td>
          <td>31.5</td>
          <td>39.4</td>
          <td>49.7</td>
          <td style="font-weight: bold;">62.9</td>
        </tr>
        <tr>
          <td>FLAIR</td>
          <td>62.0</td>
          <td>61.0</td>
          <td>61.0</td>
          <td style="font-weight: bold;">65.4</td>
          <td style="font-weight: bold;">65.4</td>
        </tr>
      </tbody>
    </table>
    <!-- <hr/>
    Our pre-training scheme had a smaller impact on FLAIR than on the TreeSatAI-TS experiment. We attribute this to the fact that only two modalities are 
    available, which decreases the supervisory power of our cross-modal contrastive objective and our multimodal reconstruction loss. -->

  <h3>Efficiency</h3>
    </p> 
    We report the best performance of different models between TreeSatAI and TreeSatAI-TS, with pre-training and fine-tuning using 100% of labels. 
    The area of the markers is proportional to the training time, broken down in pre-training and fine-tuning when applicable. 
    OmniSat is more compact, faster to train, and performs better than all evaluated models, including the DOFA foundation model.
    </p>
    <p align="center">
      <img src="https://github.com/user-attachments/assets/0e6a378a-024a-4224-ad1d-fa7171df5adf" width="800" height="350">
    </p>

  <h3>Resources</h3>
  <hr/>
  <div class="row" style="text-align: center">
    <div class="col-xs-0 col-lg-0"></div>
    <div class="col-xs-4 col-lg-4">
      <h4>Paper</h4>
      <a href="https://arxiv.org/pdf/2404.08351" style="color:inherit;">
        <img src="https://gastruc.github.io/images/paper.jpg" alt="paper.jpg" class="text-center" style="max-width:25%; border:0.15em solid;
        border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-4 col-lg-4">
      <h4>Code</h4>
      <a href="https://github.com/gastruc/OmniSat" style="color:inherit;">
        <img src="https://gastruc.github.io/images/GitHub_Logo.svg" alt="github_repo.png" class="text-center"
             style="max-width:25%; border:0.15em solid;border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-0 col-lg-0"></div>
  </div>
    <h4 style="padding-top:0.5em">BibTeX</h4>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
          @article{astruc2024omnisat,
            title={OmniSat: Self-Supervised Modality Fusion for Earth Observation},
            author={Astruc, Guillaume and Gonthier, Nicolas and Mallet, Clement and Landrieu, Loic},
            journal={ECCV},
            year={2024}
          }</pre>
      </div>
    </div>

  <!-- <h3>Further information</h3>
  <hr/> -->
  <h3>Acknowledgements</h3>
  <hr/>
  <p>
    This work was supported by ANR project READY3D ANR-19-CE23-0007,
  and was granted access to the HPC resources of IDRIS under the allocation AD011014719 made
  by GENCI. We thank Anatol Garioud and Sebastien Giordano for their help on the creation of ´
  TreeSatAI-TS and PASTIS-HD datasets. The SPOT images are opendata thanks to the Dataterra
  Dinamis initiative in the case of the "<a href="https://dinamis.data-terra.org/opendata/">Couverture France DINAMIS</a>" program. We thank Jordi
  Inglada for inspiring discussions and valuable feedback.
  </p>
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html> 