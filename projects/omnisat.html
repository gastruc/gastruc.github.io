<!DOCTYPE html>
<html lang="en">
<head>
  <title>OmniSat</title>
  <meta name="description" content="OmniSat: Self-Supervised Modality Fusion for Earth Observation.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <meta property="og:image" content="https://github.com/gastruc/OmniSat/assets/1902679/9fc20951-1cac-4891-b67f-53ed5e0675ad">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://gastruc.github.io/projects/omnisat.html/"/>
  <meta property="og:title" content="OmniSat"/>
  <meta property="og:description" content="Project page for OmniSat: Self-Supervised Modality Fusion for Earth Observation."/>

  <!--Twitter preview-->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="OmniSat: Self-Supervised Modality Fusion for Earth Observation" />
  <meta name="twitter:description" content="Project page for OmniSat: Self-Supervised Modality Fusion for Earth Observation."/>
  <meta name="twitter:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail_twitter.png">

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>OmniSat: Self-Supervised Modality Fusion for Earth Observation</h1>
    <h4>ECCV 2024</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>
        <a href="https://gastruc.github.io/"><nobr>Guillaume Astruc</nobr></a> &emsp;
        <a href="https://ngonthier.github.io/"><nobr>Nicolas Gonthier</nobr></a> &emsp;
        <a href="https://www.umr-lastig.fr/clement-mallet/"><nobr>Clément Mallet</nobr></a> &emsp;
        <a href="https://loiclandrieu.com/"><nobr>Loic Landrieu</nobr></a>
      </h4>
      LIGM, <nobr>&Eacute;cole des Ponts</nobr>, <nobr>Univ Gustave Eiffel</nobr>, CNRS,
      <nobr>Marne-la-Vall&eacute;e, France</nobr>
      Univ Gustave Eiffel, IGN, ENSG, LASTIG, France
      IGN, France
      CNES, France
    </div>
    <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
      <a href="https://arxiv.org/pdf/2006.11132.pdf" style="color:inherit">
        <i class="fa fa-file-pdf-o fa-4x"></i></a> 
    </div>
    <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
      <a href="https://github.com/gastruc/OmniSat" style="color:inherit">
        <i class="fa fa-github fa-4x"></i></a>
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="https://github.com/gastruc/OmniSat/assets/1902679/9fc20951-1cac-4891-b67f-53ed5e0675ad" alt="teaser.jpg" class="text-center" style="width: 60%; max-width: 1100px; height: auto;">
  <h3 style="text-align:center; padding-top:1rem">
    <a class="label label-info" href="https://arxiv.org/pdf/2404.08351">Paper</a>
    <a class="label label-info" href="https://github.com/gastruc/OmniSat">Code</a>
    <!-- <a class="label label-info" href="resrc/ref.bib">BibTeX</a> -->
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    The diversity and complementarity of sensors available for Earth Observations (EO) calls for developing bespoke self-supervised multimodal learning 
    approaches. However, current multimodal EO datasets and models typically focus on a single data type, either mono-date images or time series, 
    which limits their impact. To address this issue, we introduce OmniSat, a novel architecture able to merge diverse EO modalities into </b>expressive 
    features without labels by exploiting their alignment</b>. To demonstrate the advantages of our approach, we create two new multimodal datasets 
    by augmenting existing ones with new modalities. As demonstrated for three downstream tasks---forestry, land cover classification, 
    and crop mapping---OmniSat can learn rich representations without supervision, leading to state-of-the-art performances in semi- and fully 
    supervised settings. Furthermore, our multimodal pretraining scheme improves performance </b>even when only one modality is available for inference</b>. 
  </p>

  <h3>Datasets</h3>
    <table>
      <thead>
        <tr>
          <th>Dataset name</th>
          <th>Modalities</th>
          <th>Labels</th>
          <th>Link</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>PASTIS-HD</td>
          <td> <b>SPOT 6-7 (1m)</b> + S1/S2 (30-140 / year)</td>
          <td>Crop mapping (0.2m)</td>
          <td><a href="https://huggingface.co/datasets/IGNF/PASTIS-HD">huggingface</a> or <a href="https://zenodo.org/records/10908628">zenodo</a></td>
        </tr>
        <tr>
          <td>TreeSatAI-TS</td>
          <td>Aerial (0.2m) + <b>S1/S2 (10-70 / year)</b> </td>
          <td>Forestry (60m)</td>
          <td><a href="https://huggingface.co/datasets/IGNF/TreeSatAI-Time-Series">huggingface</a></td>
        </tr>
        <tr>
          <td>FLAIR</td>
          <td>aerial (0.2m) + S2 (20-114 / year)</td>
          <td>Land cover (0.2m)</td>
          <td><a href="https://huggingface.co/datasets/IGNF/FLAIR">huggingface</a></td>
        </tr>
      </tbody>
    </table>
    <br>
    </p> 
    We represent three tiles from the considered multilabel classification datasets:
    FLAIR (a), TreeSatAI-TS (b) and PASTIS-HD (c). TreeSatAI-TS is a new dataset built by replacing
    the single-date Sentinel-1 and 2 images of TreeSatAI by year-long time series. PASTIS-HD (c)
    adds VHR satellite images to PASTIS-R.
    </p>
    <p align="center">
      <img src="https://github.com/gastruc/OmniSat/assets/1902679/289c8ca5-c0fa-4c35-8a91-af827dac0509" width="500" height="250">
    </p>
<!-- 
  <h3>Video</h3>
  <hr/>
  <div class="row" style="text-align:center">
    <div class="col-xs-6 text-center">
      <h4><u>Short presentation</u> (3min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/j20MBc1hWGQ" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>

    <div class="col-xs-6 text-center">
      <h4><u>Long presentation</u> (11min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/xhLUOh5PKBA" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>
  </div> -->

  <!-- <h3>Approach</h3> -->
  <!-- <hr/>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <h4 style="margin-right: 20%"><u>DTI framework</u></h4>
    </div>
    <div class="col-xs-6">
      <h4><u>Deep transformation module 
          <img src="http://latex.codecogs.com/svg.latex?\mathcal{T}_{f_{k}}" alt="T_f_k" border="0"/></u></h4>
    </div>
  </div>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <img src="resrc/dti.png" alt="dti.png" class="text-center" style="width: 100%; max-width: 900px">
    </div>
    <div class="col-xs-6">
      <img src="resrc/deep_tsf.png" alt="deep_tsf.png" class="text-center" style="width: 90%; max-width: 900px; margin-top:
      10px">
    </div>
  </div>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <div style="width: 90%; max-width: 900px; padding-top:10px">
      <p>Given a sample <img src="http://latex.codecogs.com/svg.latex?x_i" alt="x_i" border="0"/> and prototypes 
      <img src="http://latex.codecogs.com/svg.latex?c_1" alt="c_1" border="0"/> and 
      <img src="http://latex.codecogs.com/svg.latex?c_2" alt="c_2" border="0"/>, standard clustering such as K-means 
      assigns the sample to the closest prototype. Our DTI clustering first aligns prototypes to the sample using a
      family of parametric transformations - here rotations - then picks the prototype whose alignment yields the 
      smallest distance.</p>
      </div>
    </div>
    <div class="col-xs-6">
      <div style="width: 100%; max-width: 900px; padding-top:10px">
        <p>We predict alignment with deep learning. Given an image 
      <img src="http://latex.codecogs.com/svg.latex?x_i" alt="x_i" border="0"/>, each deep parameter predictor 
      <img src="http://latex.codecogs.com/svg.latex?f_k" alt="f_k" border="0"/> predicts 
      parameters for a sequence of transformations - here affine, morphological and thin plate spline transformations -
      to align the prototype <img src="http://latex.codecogs.com/svg.latex?c_k" alt="c_k" border="0"/>
      to the query image <img src="http://latex.codecogs.com/svg.latex?x_i" alt="x_i" border="0"/>.</p>
      </div>
    </div>
  </div> -->

  <h3>Results</h3>
    <p>We perform experiments with 100% and 10-20% of labels. See below, the F1 Score results on 100% of the training data with all modalities available:</p>
    <style>
      table, th, td {
        border: 2px solid black; /* Thicker borders */
        border-collapse: collapse;
        padding: 10px;  /* Increase padding for spacing */
      }
    </style>
    <table>
      <thead>
        <tr>
          <th>F1 Score</th>
          <th>UT&T</th>
          <th>Scale-MAE</th>
          <th>OmniSat (no pretraining)</th>
          <th>OmniSat (with pretraining)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>PASTIS-HD</td>
          <td>53.5</td>
          <td>42.2</td>
          <td>59.1</td>
          <td style="font-weight: bold;">69.9</td>
        </tr>
        <tr>
          <td>TreeSatAI-TS</td>
          <td>56.7</td>
          <td>60.4</td>
          <td>73.3</td>
          <td style="font-weight: bold;">74.2</td>
        </tr>
        <tr>
          <td>FLAIR</td>
          <td>48.8</td>
          <td>70.0</td>
          <td>70.0</td>
          <td style="font-weight: bold;">73.4</td>
        </tr>
      </tbody>
    </table>
    <br>
    <p>OmniSat also improves performance even when only one modality is available for inference. F1 Score results on 100% of the training data with only S2 data available:</p>
    
    <table>
      <thead>
        <tr>
          <th>F1 Score</th>
          <th>UT&T</th>
          <th>Scale-MAE</th>
          <th>OmniSat (no pretraining)</th>
          <th>OmniSat (with pretraining)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>PASTIS-HD</td>
          <td>61.3</td>
          <td>46.1</td>
          <td>60.1</td>
          <td style="font-weight: bold;">70.8</td>
        </tr>
        <tr>
          <td>TreeSatAI-TS</td>
          <td>57.0</td>
          <td>31.5</td>
          <td>49.7</td>
          <td style="font-weight: bold;">62.9</td>
        </tr>
        <tr>
          <td>FLAIR</td>
          <td>62.0</td>
          <td>61.0</td>
          <td style="font-weight: bold;">65.4</td>
          <td style="font-weight: bold;">65.4</td>
        </tr>
      </tbody>
    </table>
  <!-- <hr/>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Standard image clustering benchmarks</u></h4>
      <img src="resrc/prototypes.jpg" alt="prototypes.jpg" class="text-center" style="width: 100%; max-width: 1000px;">
  </div>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>MegaDepth locations</u></h4>
      <img src="resrc/megadepth.jpg" alt="megadepth.jpg" class="text-center" style="width: 100%; max-width: 1000px;
      margin-top: 5px">
  </div>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>MegaDepth Florence: detailed results</u></h4>
      <p>We show the 6 best qualitatives prototypes learned using DTI clustering
      with 20 clusters for Florence location in MegaDepth dataset. For each cluster, we show the 20 samples leading to
      minimal reconstruction errors among all the samples in the cluster as well as corresponding transformed 
      prototypes. Note how it manages to model real image transformations like illumination variations and viewpoint
      changes.</p>
      <img src="resrc/firenze.jpg" alt="firenze.jpg" class="text-center" style="width: 100%; max-width: 1100px">
  </div>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem;padding-bottom:1rem;">
    <h4><u>Instagram hashtags</u></h4>
      <p>We show the 5 best qualitatives prototypes learned using DTI clustering
      with 40 clusters for different Instagram photo collections. Each collection corresponds to a large unfiltered set
      of Instagram images (from 10k to 15k) associated to a particular hashtag. Identifying visual trends or iconic
      poses in this case is very challenging as most of the images are noise. You can visualize the type of collected 
      images directly in Instagram: 
      <a href=https://www.instagram.com/explore/tags/balitemple/>#balitemple</a>,
      <a href=https://www.instagram.com/explore/tags/santaphoto/>#santaphoto</a>,
      <a href=https://www.instagram.com/explore/tags/trevifountain/>#trevifountain</a>,
      <a href=https://www.instagram.com/explore/tags/weddingkiss/>#weddingkiss</a>,
      <a href=https://www.instagram.com/explore/tags/yogahandstand/>#yogahandstand</a>.</p>
      <img src="resrc/instagram.jpg" alt="instagram.jpg" class="text-center"
           style="width:100%;max-width:850px;margin-top:1rem;padding-right:1.5rem;">
  </div> -->

  <h3>Resources</h3>
  <hr/>
  <div class="row" style="text-align: center">
    <div class="col-xs-0 col-lg-0"></div>
    <div class="col-xs-4 col-lg-4">
      <h4>Paper</h4>
      <a href="https://arxiv.org/pdf/2404.08351" style="color:inherit;">
        <img src="https://gastruc.github.io/images/paper.jpg" alt="paper.jpg" class="text-center" style="max-width:25%; border:0.15em solid;
        border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-4 col-lg-4">
      <h4>Code</h4>
      <a href="https://github.com/gastruc/OmniSat" style="color:inherit;">
        <img src="https://gastruc.github.io/images/GitHub_Logo.svg" alt="github_repo.png" class="text-center"
             style="max-width:25%; border:0.15em solid;border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-0 col-lg-0"></div>
  </div>
    <h4 style="padding-top:0.5em">BibTeX</h4>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
          @article{astruc2024omnisat,
            title={OmniSat: Self-Supervised Modality Fusion for Earth Observation},
            author={Astruc, Guillaume and Gonthier, Nicolas and Mallet, Clement and Landrieu, Loic},
            journal={arXiv preprint arXiv:2404.08351},
            year={2024}
          }</pre>
      </div>
    </div>

  <!-- <h3>Further information</h3>
  <hr/> -->
  <h3>Acknowledgements</h3>
  <hr/>
  <p>
    This work was supported by ANR project READY3D ANR-19-CE23-0007,
  and was granted access to the HPC resources of IDRIS under the allocation AD011014719 made
  by GENCI. We thank Anatol Garioud and Sebastien Giordano for their help on the creation of ´
  TreeSatAI-TS and PASTIS-HD datasets. The SPOT images are opendata thanks to the Dataterra
  Dinamis initiative in the case of the "<a href="https://dinamis.data-terra.org/opendata/">Couverture France DINAMIS</a>" program. We thank Jordi
  Inglada for inspiring discussions and valuable feedback.
  </p>
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html> 